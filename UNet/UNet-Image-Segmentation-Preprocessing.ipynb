{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-01-06T11:35:49.855190Z",
     "iopub.status.busy": "2025-01-06T11:35:49.854989Z",
     "iopub.status.idle": "2025-01-06T11:36:03.277664Z",
     "shell.execute_reply": "2025-01-06T11:36:03.276514Z",
     "shell.execute_reply.started": "2025-01-06T11:35:49.855170Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting segmentation-models-pytorch\n",
      "  Downloading segmentation_models_pytorch-0.3.4-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting efficientnet-pytorch==0.7.1 (from segmentation-models-pytorch)\n",
      "  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: huggingface-hub>=0.24.6 in /usr/local/lib/python3.10/dist-packages (from segmentation-models-pytorch) (0.24.7)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from segmentation-models-pytorch) (10.4.0)\n",
      "Collecting pretrainedmodels==0.7.4 (from segmentation-models-pytorch)\n",
      "  Downloading pretrainedmodels-0.7.4.tar.gz (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from segmentation-models-pytorch) (1.16.0)\n",
      "Collecting timm==0.9.7 (from segmentation-models-pytorch)\n",
      "  Downloading timm-0.9.7-py3-none-any.whl.metadata (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torchvision>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from segmentation-models-pytorch) (0.19.1+cu121)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from segmentation-models-pytorch) (4.66.5)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (2.4.1+cu121)\n",
      "Collecting munch (from pretrainedmodels==0.7.4->segmentation-models-pytorch)\n",
      "  Downloading munch-4.0.0-py2.py3-none-any.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm==0.9.7->segmentation-models-pytorch) (6.0.2)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm==0.9.7->segmentation-models-pytorch) (0.4.5)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24.6->segmentation-models-pytorch) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24.6->segmentation-models-pytorch) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24.6->segmentation-models-pytorch) (24.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24.6->segmentation-models-pytorch) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24.6->segmentation-models-pytorch) (4.12.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.5.0->segmentation-models-pytorch) (1.26.4)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (3.1.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.24.6->segmentation-models-pytorch) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.24.6->segmentation-models-pytorch) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.24.6->segmentation-models-pytorch) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.24.6->segmentation-models-pytorch) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (1.3.0)\n",
      "Downloading segmentation_models_pytorch-0.3.4-py3-none-any.whl (109 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.5/109.5 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading timm-0.9.7-py3-none-any.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading munch-4.0.0-py2.py3-none-any.whl (9.9 kB)\n",
      "Building wheels for collected packages: efficientnet-pytorch, pretrainedmodels\n",
      "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16425 sha256=973c57d0de7212f701ea66c8f90fe19e9e387f789c6e11e451ec893bf8f85b8a\n",
      "  Stored in directory: /root/.cache/pip/wheels/03/3f/e9/911b1bc46869644912bda90a56bcf7b960f20b5187feea3baf\n",
      "  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60944 sha256=d74d9df1783867c9910d9b23b79dfa0f9509a9d7b66dad60947a0d211fbd3ee8\n",
      "  Stored in directory: /root/.cache/pip/wheels/35/cb/a5/8f534c60142835bfc889f9a482e4a67e0b817032d9c6883b64\n",
      "Successfully built efficientnet-pytorch pretrainedmodels\n",
      "Installing collected packages: munch, efficientnet-pytorch, timm, pretrainedmodels, segmentation-models-pytorch\n",
      "  Attempting uninstall: timm\n",
      "    Found existing installation: timm 1.0.12\n",
      "    Uninstalling timm-1.0.12:\n",
      "      Successfully uninstalled timm-1.0.12\n",
      "Successfully installed efficientnet-pytorch-0.7.1 munch-4.0.0 pretrainedmodels-0.7.4 segmentation-models-pytorch-0.3.4 timm-0.9.7\n",
      "Requirement already satisfied: torchinfo in /usr/local/lib/python3.10/dist-packages (1.8.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install segmentation-models-pytorch\n",
    "!pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-06T11:36:06.145933Z",
     "iopub.status.busy": "2025-01-06T11:36:06.145608Z",
     "iopub.status.idle": "2025-01-06T11:36:15.114615Z",
     "shell.execute_reply": "2025-01-06T11:36:15.113891Z",
     "shell.execute_reply.started": "2025-01-06T11:36:06.145906Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 1.4.24 (you have 1.4.15). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import jaccard_score\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchinfo import summary\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "from segmentation_models_pytorch.losses import DiceLoss, JaccardLoss\n",
    "\n",
    "# Constants\n",
    "IMAGE_SIZE = (224, 224)\n",
    "SUPPORTED_FORMATS = ['.jpg', '.jpeg', '.png']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-06T11:36:22.818984Z",
     "iopub.status.busy": "2025-01-06T11:36:22.818697Z",
     "iopub.status.idle": "2025-01-06T11:36:22.829913Z",
     "shell.execute_reply": "2025-01-06T11:36:22.828993Z",
     "shell.execute_reply.started": "2025-01-06T11:36:22.818965Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, kernel_size=3,stride=1,padding=1)\n",
    "        self.relu  = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, kernel_size=3,stride=1,padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.relu(self.conv2(self.relu(self.conv1(x)))).to(device)\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, chs=(3,64,128,256,512,1024)):\n",
    "        super().__init__()\n",
    "        self.enc_blocks = nn.ModuleList([Block(chs[i], chs[i+1]) for i in range(len(chs)-1)])\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ftrs = []\n",
    "        for block in self.enc_blocks:\n",
    "            x = block(x)\n",
    "            ftrs.append(x)\n",
    "            x = self.pool(x)\n",
    "        return ftrs\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, chs=(1024, 512, 256, 128, 64)):\n",
    "        super().__init__()\n",
    "        self.chs  = chs\n",
    "        self.upconvs  = nn.ModuleList([nn.ConvTranspose2d(chs[i], chs[i+1], 2, 2) for i in range(len(chs)-1)])\n",
    "        self.dec_blocks = nn.ModuleList([Block(chs[i], chs[i+1]) for i in range(len(chs)-1)])\n",
    "\n",
    "    def forward(self, x, encoder_features):\n",
    "        for i in range(len(self.chs)-1):\n",
    "            x  = self.upconvs[i](x)\n",
    "            enc_ftrs = self.crop(encoder_features[i], x)\n",
    "            x = torch.cat([x, enc_ftrs], dim=1)\n",
    "            x = self.dec_blocks[i](x)\n",
    "        return x\n",
    "\n",
    "    def crop(self, enc_ftrs, x):\n",
    "        _, _, H, W = x.shape\n",
    "        enc_ftrs = transforms.CenterCrop([H, W])(enc_ftrs)\n",
    "        return enc_ftrs\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, enc_chs=(3,64,128,256,512,1024), dec_chs=(1024, 512, 256, 128, 64), num_class=1):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(enc_chs)\n",
    "        self.decoder  = Decoder(dec_chs)\n",
    "        self.head  = nn.Conv2d(dec_chs[-1], num_class, 1)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc_ftrs = self.encoder(x)\n",
    "        out = self.decoder(enc_ftrs[::-1][0], enc_ftrs[::-1][1:])\n",
    "        out = self.head(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-06T11:36:27.652359Z",
     "iopub.status.busy": "2025-01-06T11:36:27.652072Z",
     "iopub.status.idle": "2025-01-06T11:36:27.660037Z",
     "shell.execute_reply": "2025-01-06T11:36:27.658883Z",
     "shell.execute_reply.started": "2025-01-06T11:36:27.652338Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "class MaskGenerator:\n",
    "    def __init__(self, model, device, input_size=(256, 256)):\n",
    "        \"\"\"\n",
    "        Initialize the mask generator with a pre-trained UNet model\n",
    "        \n",
    "        Args:\n",
    "            model: Pre-trained UNet model\n",
    "            device: torch.device to use for computation\n",
    "            input_size: Tuple of (height, width) for input image resizing\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.input_size = input_size\n",
    "        \n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(input_size),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "    \n",
    "    def preprocess_image(self, image):\n",
    "        \"\"\"\n",
    "        Preprocess the input image for the model\n",
    "        \n",
    "        Args:\n",
    "            image: PIL Image or numpy array\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Preprocessed image tensor\n",
    "        \"\"\"\n",
    "        if isinstance(image, np.ndarray):\n",
    "            image = Image.fromarray(image)\n",
    "        \n",
    "        # Apply transformations\n",
    "        image_tensor = self.transform(image)\n",
    "        return image_tensor.unsqueeze(0).to(self.device)  # Add batch dimension and move to device\n",
    "    \n",
    "    def postprocess_mask(self, mask_tensor):\n",
    "        \"\"\"\n",
    "        Postprocess the mask tensor to get the final binary mask\n",
    "        \n",
    "        Args:\n",
    "            mask_tensor: Tensor output from the model\n",
    "            \n",
    "        Returns:\n",
    "            numpy.ndarray: Binary mask array\n",
    "        \"\"\"\n",
    "        # Convert to numpy and ensure proper dimensions\n",
    "        mask = mask_tensor.squeeze(0).cpu().detach().permute(1, 2, 0).numpy()\n",
    "        \n",
    "        # Apply thresholding to get binary mask\n",
    "        mask[mask < 0] = 0\n",
    "        mask[mask > 0] = 1\n",
    "        \n",
    "        return mask\n",
    "\n",
    "    def generate_mask(self, image):\n",
    "        \"\"\"\n",
    "        Generate binary mask for the input image using the UNet model\n",
    "        \n",
    "        Args:\n",
    "            image: PIL Image or numpy array\n",
    "            \n",
    "        Returns:\n",
    "            numpy.ndarray: Binary mask array\n",
    "        \"\"\"\n",
    "        # Ensure model is in eval mode\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Preprocess image\n",
    "        image_tensor = self.preprocess_image(image)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Get model prediction\n",
    "            mask_tensor = self.model(image_tensor)\n",
    "            \n",
    "            # Postprocess the mask\n",
    "            mask = self.postprocess_mask(mask_tensor)\n",
    "            \n",
    "        return mask\n",
    "    \n",
    "    def generate_batch_masks(self, images):\n",
    "        \"\"\"\n",
    "        Generate masks for a batch of images\n",
    "        \n",
    "        Args:\n",
    "            images: List of PIL Images or numpy arrays\n",
    "            \n",
    "        Returns:\n",
    "            List[numpy.ndarray]: List of mask arrays\n",
    "        \"\"\"\n",
    "        return [self.generate_mask(img) for img in images]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-06T11:36:32.767667Z",
     "iopub.status.busy": "2025-01-06T11:36:32.767302Z",
     "iopub.status.idle": "2025-01-06T11:36:32.773327Z",
     "shell.execute_reply": "2025-01-06T11:36:32.772425Z",
     "shell.execute_reply.started": "2025-01-06T11:36:32.767637Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ImagePreprocessor:\n",
    "    \"\"\"Skin Cancer Image preprocessing pipeline\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def hair_remove(image):\n",
    "        \"\"\"Remove hair from skin images\"\"\"\n",
    "        try:\n",
    "            grayScale = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "            kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (17, 17))\n",
    "            blackhat = cv2.morphologyEx(grayScale, cv2.MORPH_BLACKHAT, kernel)\n",
    "            _, threshold = cv2.threshold(blackhat, 10, 255, cv2.THRESH_BINARY)\n",
    "            final_image = cv2.inpaint(image, threshold, 1, cv2.INPAINT_TELEA)\n",
    "            return final_image\n",
    "        except Exception as e:\n",
    "            print(f\"Error in hair removal: {str(e)}\")\n",
    "            return image\n",
    "\n",
    "    @staticmethod\n",
    "    def sharpen_image(image):\n",
    "        \"\"\"Sharpen image using unsharp masking\"\"\"\n",
    "        gaussian = cv2.GaussianBlur(image, (0, 0), 2.0)\n",
    "        return cv2.addWeighted(image, 1.5, gaussian, -0.5, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-06T11:36:36.366448Z",
     "iopub.status.busy": "2025-01-06T11:36:36.366150Z",
     "iopub.status.idle": "2025-01-06T11:36:36.370760Z",
     "shell.execute_reply": "2025-01-06T11:36:36.369805Z",
     "shell.execute_reply.started": "2025-01-06T11:36:36.366420Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def preprocess_image(image, target_size=(224, 224)):\n",
    "    \"\"\"Apply all preprocessing steps to an image\"\"\"\n",
    "    preprocessor = ImagePreprocessor()\n",
    "    \n",
    "    image = preprocessor.hair_remove(image)\n",
    "    image = preprocessor.sharpen_image(image)\n",
    "    image = cv2.resize(image, target_size, interpolation=cv2.INTER_NEAREST)\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-06T11:36:39.367076Z",
     "iopub.status.busy": "2025-01-06T11:36:39.366795Z",
     "iopub.status.idle": "2025-01-06T11:36:39.374850Z",
     "shell.execute_reply": "2025-01-06T11:36:39.373841Z",
     "shell.execute_reply.started": "2025-01-06T11:36:39.367055Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def process_and_organize_dataset(source_path, destination_path, model, device):\n",
    "    \"\"\"Process images and organize them into the new structure\"\"\"\n",
    "    source_path = Path(source_path)\n",
    "    destination_path = Path(destination_path)\n",
    "    \n",
    "    mask_generator = MaskGenerator(model, device)\n",
    "    \n",
    "    splits = ['train_directory', 'test_directory', 'validation_directory']\n",
    "    for split in splits:\n",
    "        split_path = source_path / split\n",
    "        dest_split = split\n",
    "        \n",
    "        for category in ['nv', 'mel', 'bkl', 'bcc', 'akiec', 'vasc', 'df']:\n",
    "            category_path = split_path / category\n",
    "            if not category_path.exists():\n",
    "                continue\n",
    "                \n",
    "            print(f\"Processing {split}/{category}...\")\n",
    "            \n",
    "            # Process each image in the category\n",
    "            for img_file in tqdm([f for ext in SUPPORTED_FORMATS for f in category_path.glob(f'*{ext}')]):\n",
    "                img = cv2.imread(str(img_file))\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                \n",
    "                # Apply preprocessing\n",
    "                processed_img = preprocess_image(img.copy(), target_size=(224, 224))\n",
    "                \n",
    "                # Generate filename without extension\n",
    "                filename = img_file.stem\n",
    "                \n",
    "                # Create and save ground truth mask from preprocessed image\n",
    "                mask = mask_generator.generate_mask(processed_img)\n",
    "                if processed_img.shape[:2] != mask.shape[:2]:\n",
    "                    mask = cv2.resize(mask, processed_img.shape[:2][::-1], interpolation=cv2.INTER_NEAREST)\n",
    "                if len(mask.shape) == 2:\n",
    "                    mask = np.expand_dims(mask, axis=2)\n",
    "                mask_save = (mask * 255).astype(np.uint8)\n",
    "                mask_path = destination_path / 'ground_truth' / dest_split / category / f\"{filename}.jpg\"\n",
    "                cv2.imwrite(str(mask_path), mask_save)\n",
    "                \n",
    "                # Create and save segmented image\n",
    "                segmented = processed_img.copy()\n",
    "                segmented[mask[:, :, 0] == 0] = 0\n",
    "                segmented_path = destination_path / 'unet_segmented' / dest_split / category / f\"{filename}.jpg\"\n",
    "                cv2.imwrite(str(segmented_path), cv2.cvtColor(segmented, cv2.COLOR_RGB2BGR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-06T11:44:47.018263Z",
     "iopub.status.busy": "2025-01-06T11:44:47.017928Z",
     "iopub.status.idle": "2025-01-06T11:44:47.025191Z",
     "shell.execute_reply": "2025-01-06T11:44:47.024333Z",
     "shell.execute_reply.started": "2025-01-06T11:44:47.018236Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "base_directory = '/kaggle/working/unet_segmented_images'\n",
    "os.mkdir(base_directory)\n",
    "\n",
    "subfolders = ['ground_truth', 'unet_segmented']\n",
    "directory = ['train_directory', 'test_directory', 'validation_directory']\n",
    "classes = ['nv', 'mel', 'bkl', 'bcc', 'akiec', 'vasc', 'df']\n",
    "\n",
    "for subf in subfolders:\n",
    "    path = os.path.join(base_directory, subf)\n",
    "    os.mkdir(path)\n",
    "    for dirc in directory:\n",
    "        path = os.path.join(base_directory, subf, dirc)\n",
    "        os.mkdir(path)\n",
    "        for cls in classes:\n",
    "            path = os.path.join(base_directory, subf, dirc, cls)\n",
    "            os.mkdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-06T11:45:37.594531Z",
     "iopub.status.busy": "2025-01-06T11:45:37.594102Z",
     "iopub.status.idle": "2025-01-06T11:45:38.082639Z",
     "shell.execute_reply": "2025-01-06T11:45:38.081752Z",
     "shell.execute_reply.started": "2025-01-06T11:45:37.594497Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-ea199026fbea>:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  unet_model.load_state_dict(torch.load(model_path,  map_location=torch.device('cpu')))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "UNet(\n",
       "  (encoder): Encoder(\n",
       "    (enc_blocks): ModuleList(\n",
       "      (0): Block(\n",
       "        (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (relu): ReLU()\n",
       "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (1): Block(\n",
       "        (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (relu): ReLU()\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (2): Block(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (relu): ReLU()\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (3): Block(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (relu): ReLU()\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (upconvs): ModuleList(\n",
       "      (0): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (1): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (2): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
       "    )\n",
       "    (dec_blocks): ModuleList(\n",
       "      (0): Block(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (relu): ReLU()\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (1): Block(\n",
       "        (conv1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (relu): ReLU()\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (2): Block(\n",
       "        (conv1): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (relu): ReLU()\n",
       "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (head): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_path = \"/kaggle/input/multiclassskincancer\"\n",
    "destination_path = \"/kaggle/working/unet_segmented_images\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "unet_model = UNet(enc_chs=(3,32, 64,128,256), dec_chs=(256, 128, 64, 32), num_class=1)\n",
    "model_path = '/kaggle/input/image-segmentation-unet-model/segmentation_model.pth'\n",
    "unet_model.load_state_dict(torch.load(model_path,  map_location=torch.device('cpu')))\n",
    "unet_model.eval()\n",
    "unet_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-06T11:47:12.177655Z",
     "iopub.status.busy": "2025-01-06T11:47:12.177312Z",
     "iopub.status.idle": "2025-01-06T12:07:20.127889Z",
     "shell.execute_reply": "2025-01-06T12:07:20.126825Z",
     "shell.execute_reply.started": "2025-01-06T11:47:12.177626Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing train_directory/nv...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "399f60265b46460e8665dd000cafcd73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5115 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing train_directory/mel...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5131162f17e45d0b65e35b06f9b5485",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5950 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing train_directory/bkl...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d40231d4833a4f8ebd29d0fc5f2dd208",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5990 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing train_directory/bcc...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61ff5bc338634b398360df6660056b52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5462 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing train_directory/akiec...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fc172ddc5be493588a7fe2119de024a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5510 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing train_directory/vasc...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e19227cd0f55456597d67649f652dc90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4810 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing train_directory/df...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "338647abbb5f4df9acfc0f7de907bab8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4090 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing test_directory/nv...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24369c81e9484cc18947c41cedb9ec27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/883 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing test_directory/mel...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42412442468d4869a301403f14cf404a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/46 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing test_directory/bkl...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba92f5eeb80740dcb9f2bd27643654cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/88 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing test_directory/bcc...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20ddda97805846628bc9deb457ef2372",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing test_directory/akiec...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ad0f4ad95574cbfaa127982338420f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing test_directory/vasc...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d40e5e9754c46d69d8c01ec73c3e4ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing test_directory/df...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd55784c516341feb0d4bff535d0b658",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing validation_directory/nv...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ffcaa12cdc9417f97c6557597de44f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/707 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing validation_directory/mel...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f590c911747e40848b5acf0ec08678bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing validation_directory/bkl...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cffb6e24b6c48d0bf0bf9ec68c73cd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/71 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing validation_directory/bcc...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55b931cb71884012b1ae683f0bfe6525",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing validation_directory/akiec...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c278e5df0f5142e687a55766dc329de0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing validation_directory/vasc...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1229190685f142339f56d6ff5456f0e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing validation_directory/df...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60378c2b83a142fe914476917ce0c5be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "process_and_organize_dataset(source_path, destination_path, unet_model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-06T12:12:19.668415Z",
     "iopub.status.busy": "2025-01-06T12:12:19.668118Z",
     "iopub.status.idle": "2025-01-06T12:12:19.726143Z",
     "shell.execute_reply": "2025-01-06T12:12:19.725263Z",
     "shell.execute_reply.started": "2025-01-06T12:12:19.668393Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ground_truth\n",
      "train_directory\n",
      "nv    :  5115\n",
      "mel    :  5950\n",
      "bkl    :  5990\n",
      "bcc    :  5462\n",
      "akiec    :  5510\n",
      "vasc    :  4810\n",
      "df    :  4090\n",
      "test_directory\n",
      "nv    :  883\n",
      "mel    :  46\n",
      "bkl    :  88\n",
      "bcc    :  35\n",
      "akiec    :  30\n",
      "vasc    :  13\n",
      "df    :  8\n",
      "validation_directory\n",
      "nv    :  707\n",
      "mel    :  37\n",
      "bkl    :  71\n",
      "bcc    :  28\n",
      "akiec    :  24\n",
      "vasc    :  10\n",
      "df    :  6\n",
      "unet_segmented\n",
      "train_directory\n",
      "nv    :  5115\n",
      "mel    :  5950\n",
      "bkl    :  5990\n",
      "bcc    :  5462\n",
      "akiec    :  5510\n",
      "vasc    :  4810\n",
      "df    :  4090\n",
      "test_directory\n",
      "nv    :  883\n",
      "mel    :  46\n",
      "bkl    :  88\n",
      "bcc    :  35\n",
      "akiec    :  30\n",
      "vasc    :  13\n",
      "df    :  8\n",
      "validation_directory\n",
      "nv    :  707\n",
      "mel    :  37\n",
      "bkl    :  71\n",
      "bcc    :  28\n",
      "akiec    :  24\n",
      "vasc    :  10\n",
      "df    :  6\n"
     ]
    }
   ],
   "source": [
    "base_dir = '/kaggle/working/unet_segmented_images'\n",
    "subfolders = ['ground_truth', 'unet_segmented']\n",
    "directory = ['train_directory', 'test_directory', 'validation_directory']\n",
    "classes = ['nv', 'mel', 'bkl', 'bcc', 'akiec', 'vasc', 'df']\n",
    "\n",
    "for subf in subfolders:\n",
    "    print(subf)\n",
    "    for dirc in directory:\n",
    "        print(dirc)\n",
    "        for cls in classes:\n",
    "            path = os.path.join(base_dir, subf, dirc, cls)\n",
    "            print(f\"{cls}    : \", len(os.listdir(path)))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6428166,
     "sourceId": 10377356,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6018704,
     "sourceId": 9816642,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30823,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
