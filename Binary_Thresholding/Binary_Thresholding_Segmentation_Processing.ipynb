{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10377356,"sourceType":"datasetVersion","datasetId":6428166}],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nimport torch\nimport torchvision\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nfrom pathlib import Path\n\n# Constants\nIMAGE_SIZE = (224, 224)\nSUPPORTED_FORMATS = ['.jpg', '.jpeg', '.png']","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-06T16:46:56.396130Z","iopub.execute_input":"2025-01-06T16:46:56.396524Z","iopub.status.idle":"2025-01-06T16:47:02.818868Z","shell.execute_reply.started":"2025-01-06T16:46:56.396494Z","shell.execute_reply":"2025-01-06T16:47:02.817174Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"class ImagePreprocessor:\n    \"\"\"Skin Cancer Image preprocessing pipeline\"\"\"\n    \n    @staticmethod\n    def hair_remove(image):\n        \"\"\"Remove hair from skin images\"\"\"\n        try:\n            grayScale = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n            kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (17, 17))\n            blackhat = cv2.morphologyEx(grayScale, cv2.MORPH_BLACKHAT, kernel)\n            _, threshold = cv2.threshold(blackhat, 10, 255, cv2.THRESH_BINARY)\n            final_image = cv2.inpaint(image, threshold, 1, cv2.INPAINT_TELEA)\n            return final_image\n        except Exception as e:\n            print(f\"Error in hair removal: {str(e)}\")\n            return image\n\n    @staticmethod\n    def sharpen_image(image):\n        \"\"\"Sharpen image using unsharp masking\"\"\"\n        gaussian = cv2.GaussianBlur(image, (0, 0), 2.0)\n        return cv2.addWeighted(image, 1.5, gaussian, -0.5, 0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T16:47:10.994421Z","iopub.execute_input":"2025-01-06T16:47:10.994865Z","iopub.status.idle":"2025-01-06T16:47:11.002962Z","shell.execute_reply.started":"2025-01-06T16:47:10.994834Z","shell.execute_reply":"2025-01-06T16:47:11.001264Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"def preprocess_image(image, target_size=(224, 224)):\n    \"\"\"Apply all preprocessing steps to an image\"\"\"\n    preprocessor = ImagePreprocessor()\n    \n    image = preprocessor.hair_remove(image)\n    image = preprocessor.sharpen_image(image)\n    image = cv2.resize(image, target_size, interpolation=cv2.INTER_NEAREST)\n    \n    return image","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T16:47:15.544744Z","iopub.execute_input":"2025-01-06T16:47:15.545164Z","iopub.status.idle":"2025-01-06T16:47:15.550873Z","shell.execute_reply.started":"2025-01-06T16:47:15.545110Z","shell.execute_reply":"2025-01-06T16:47:15.549403Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"def create_binary_mask(image):\n    \"\"\"Create binary mask from image using binary thresholding\"\"\"\n    # Convert to grayscale\n    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n    \n    # Apply gaussian blur\n    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n    \n    # Apply thresholding\n    _, binary = cv2.threshold(blurred, 0, 1, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n    \n    # Invert the mask (1 for lesion, 0 for background)\n    mask = 1 - binary\n    \n    # Clean up mask with morphological operations\n    kernel = np.ones((5,5), np.uint8)\n    mask = cv2.morphologyEx(mask.astype(np.uint8), cv2.MORPH_CLOSE, kernel)\n    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n    \n    return mask","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T16:47:19.365479Z","iopub.execute_input":"2025-01-06T16:47:19.365905Z","iopub.status.idle":"2025-01-06T16:47:19.372737Z","shell.execute_reply.started":"2025-01-06T16:47:19.365874Z","shell.execute_reply":"2025-01-06T16:47:19.371163Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def process_and_organize_dataset(source_path, destination_path):\n    \"\"\"Process images and organize them into the new structure\"\"\"\n    source_path = Path(source_path)\n    destination_path = Path(destination_path)\n    \n    # Process each split (train, test, val)\n    splits = ['train_directory', 'test_directory', 'validation_directory']\n    for split in splits:\n        split_path = source_path / split\n        \n        dest_split = split\n        \n        for category in ['nv', 'mel', 'bkl', 'bcc', 'akiec', 'vasc', 'df']:\n            category_path = split_path / category\n            if not category_path.exists():\n                continue\n                \n            print(f\"Processing {split}/{category}...\")\n            \n            # Process each image in the category\n            for img_file in tqdm([f for ext in SUPPORTED_FORMATS for f in category_path.glob(f'*{ext}')]):\n                img = cv2.imread(str(img_file))\n                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n                \n                # Apply preprocessing\n                processed_img = preprocess_image(img.copy(), target_size=(224, 224))\n                \n                # Generate filename without extension\n                filename = img_file.stem\n                \n                # Create and save binary thresholding ground truth mask from preprocessed image\n                mask = create_binary_mask(processed_img)\n                mask_save = (mask * 255).astype(np.uint8)\n                mask_path = destination_path / 'ground_truth' / dest_split / category / f\"{filename}.jpg\"\n                cv2.imwrite(str(mask_path), mask_save)\n                \n                # Create and save binary thresholding segmented image\n                segmented = processed_img.copy()\n                segmented[mask == 0] = 0\n                segmented_path = destination_path / 'segmented_bt' / dest_split / category / f\"{filename}.jpg\"\n                cv2.imwrite(str(segmented_path), cv2.cvtColor(segmented, cv2.COLOR_RGB2BGR))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T16:47:22.499176Z","iopub.execute_input":"2025-01-06T16:47:22.499637Z","iopub.status.idle":"2025-01-06T16:47:22.508733Z","shell.execute_reply.started":"2025-01-06T16:47:22.499601Z","shell.execute_reply":"2025-01-06T16:47:22.507344Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"base_directory = '/kaggle/working/segmented_bt_images'\nos.mkdir(base_directory)\n\nsubfolders = ['ground_truth', 'segmented_bt']\ndirectory = ['train_directory', 'test_directory', 'validation_directory']\nclasses = ['nv', 'mel', 'bkl', 'bcc', 'akiec', 'vasc', 'df']\n\nfor subf in subfolders:\n    path = os.path.join(base_directory, subf)\n    os.mkdir(path)\n    for dirc in directory:\n        path = os.path.join(base_directory, subf, dirc)\n        os.mkdir(path)\n        for cls in classes:\n            path = os.path.join(base_directory, subf, dirc, cls)\n            os.mkdir(path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T16:47:36.863380Z","iopub.execute_input":"2025-01-06T16:47:36.863723Z","iopub.status.idle":"2025-01-06T16:47:36.873254Z","shell.execute_reply.started":"2025-01-06T16:47:36.863696Z","shell.execute_reply":"2025-01-06T16:47:36.871807Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"source_path = \"/kaggle/input/multiclassskincancer\"\ndestination_path = \"/kaggle/working/segmented_bt_images\"\nprocess_and_organize_dataset(source_path, destination_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T16:48:07.483914Z","iopub.execute_input":"2025-01-06T16:48:07.484337Z","iopub.status.idle":"2025-01-06T17:10:49.094857Z","shell.execute_reply.started":"2025-01-06T16:48:07.484307Z","shell.execute_reply":"2025-01-06T17:10:49.093367Z"}},"outputs":[{"name":"stdout","text":"Processing train_directory/nv...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5115/5115 [05:07<00:00, 16.62it/s]\n","output_type":"stream"},{"name":"stdout","text":"Processing train_directory/mel...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5950/5950 [02:53<00:00, 34.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Processing train_directory/bkl...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5990/5990 [03:19<00:00, 30.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"Processing train_directory/bcc...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5462/5462 [02:39<00:00, 34.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"Processing train_directory/akiec...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5510/5510 [02:41<00:00, 34.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"Processing train_directory/vasc...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 4810/4810 [01:49<00:00, 44.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"Processing train_directory/df...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 4090/4090 [01:53<00:00, 36.06it/s]\n","output_type":"stream"},{"name":"stdout","text":"Processing test_directory/nv...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 883/883 [00:59<00:00, 14.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"Processing test_directory/mel...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 46/46 [00:04<00:00, 11.43it/s]\n","output_type":"stream"},{"name":"stdout","text":"Processing test_directory/bkl...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 88/88 [00:07<00:00, 12.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Processing test_directory/bcc...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 35/35 [00:02<00:00, 16.06it/s]\n","output_type":"stream"},{"name":"stdout","text":"Processing test_directory/akiec...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 30/30 [00:02<00:00, 13.58it/s]\n","output_type":"stream"},{"name":"stdout","text":"Processing test_directory/vasc...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 13/13 [00:00<00:00, 17.06it/s]\n","output_type":"stream"},{"name":"stdout","text":"Processing test_directory/df...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 8/8 [00:00<00:00, 10.66it/s]\n","output_type":"stream"},{"name":"stdout","text":"Processing validation_directory/nv...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 707/707 [00:46<00:00, 15.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"Processing validation_directory/mel...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 37/37 [00:02<00:00, 14.06it/s]\n","output_type":"stream"},{"name":"stdout","text":"Processing validation_directory/bkl...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 71/71 [00:05<00:00, 12.35it/s]\n","output_type":"stream"},{"name":"stdout","text":"Processing validation_directory/bcc...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 28/28 [00:01<00:00, 15.80it/s]\n","output_type":"stream"},{"name":"stdout","text":"Processing validation_directory/akiec...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 24/24 [00:01<00:00, 12.83it/s]\n","output_type":"stream"},{"name":"stdout","text":"Processing validation_directory/vasc...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 10/10 [00:00<00:00, 14.83it/s]\n","output_type":"stream"},{"name":"stdout","text":"Processing validation_directory/df...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 6/6 [00:00<00:00, 12.44it/s]\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"base_dir = '/kaggle/working/segmented_bt_images'\nsubfolders = ['ground_truth', 'segmented_bt']\ndirectory = ['train_directory', 'test_directory', 'validation_directory']\nclasses = ['nv', 'mel', 'bkl', 'bcc', 'akiec', 'vasc', 'df']\n\nfor subf in subfolders:\n    print(subf)\n    for dirc in directory:\n        print(dirc)\n        for cls in classes:\n            path = os.path.join(base_dir, subf, dirc, cls)\n            print(f\"{cls}    : \", len(os.listdir(path)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T17:10:58.511216Z","iopub.execute_input":"2025-01-06T17:10:58.511693Z","iopub.status.idle":"2025-01-06T17:10:58.587371Z","shell.execute_reply.started":"2025-01-06T17:10:58.511658Z","shell.execute_reply":"2025-01-06T17:10:58.586255Z"}},"outputs":[{"name":"stdout","text":"ground_truth\ntrain_directory\nnv    :  5115\nmel    :  5950\nbkl    :  5990\nbcc    :  5462\nakiec    :  5510\nvasc    :  4810\ndf    :  4090\ntest_directory\nnv    :  883\nmel    :  46\nbkl    :  88\nbcc    :  35\nakiec    :  30\nvasc    :  13\ndf    :  8\nvalidation_directory\nnv    :  707\nmel    :  37\nbkl    :  71\nbcc    :  28\nakiec    :  24\nvasc    :  10\ndf    :  6\nsegmented_bt\ntrain_directory\nnv    :  5115\nmel    :  5950\nbkl    :  5990\nbcc    :  5462\nakiec    :  5510\nvasc    :  4810\ndf    :  4090\ntest_directory\nnv    :  883\nmel    :  46\nbkl    :  88\nbcc    :  35\nakiec    :  30\nvasc    :  13\ndf    :  8\nvalidation_directory\nnv    :  707\nmel    :  37\nbkl    :  71\nbcc    :  28\nakiec    :  24\nvasc    :  10\ndf    :  6\n","output_type":"stream"}],"execution_count":9}]}